{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "96a5d84734c2fcc0f1c4e29111ed2080412c48bc33f767562c76143427b08200"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Tobis Twitter Network\n",
    "\n",
    "This is a project about Tobis Twitter Network. The question we asked ourself is: \"Which people (accounts) are influencing Tobis Feed the most?\". To answer the question we are using methods from the Social Network Analysis. Our first approach is to collect all friends (people which Tobi is following) and check which of his friends has overlapping friends. The second approach is to just create a network with all friends and friends of friends (2nd grade friends) from Tobi and calculate the centrallity betweenness of each node."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1. Crawling\n",
    "To crawling twitter we are using the [tweepy library](https://www.tweepy.org/). To use this library it is necessary to get an API-key from [Twitter Developer](https://developer.twitter.com/en). Our API-keys is stored in `config.py`, there is a template in the repository."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string # for printable comparison\n",
    "import time # time thread-timeout\n",
    "import datetime # get current time\n",
    "import config # for apikeys\n",
    "import tweepy # for crawling Twitter"
   ]
  },
  {
   "source": [
    "First we must authenticate us with our API-keys:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Authenticate Tweepy, a Python Library for crawling Twitter via the Twitter API\n",
    "# Get Apikey from here: https://developer.twitter.com/en\n",
    "auth = tweepy.AppAuthHandler(\n",
    "    config.APIKEY,\n",
    "    config.SECRET_APIKEY)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "source": [
    "## Get data from Twitter API\n",
    "Now we want to access Tobis Twitter-ID (Should be `3490529422`):"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get my ID\n",
    "myName = 'tobiashoelzer'\n",
    "myID = api.get_user(myName).id # get_user returns a huge User Object with name, id, etc.\n",
    "\n",
    "myID"
   ]
  },
  {
   "source": [
    "Wow! An ID! How awesome! Now we crawl Tobis friends, theire IDs and Names:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get my follows (friends)\n",
    "myFriendsIDs = api.friends_ids(myID) # Returns a list with the IDs of max. 100 friends\n",
    "myFriendsNames = {} # Dict where each friend ID is mapped to his name\n",
    "\n",
    "# F is for Friends who do stuff together. U is for You and me. N is for Anywhere and anytime at all. Down here in the deep blue sea! - so 'f_id' means 'friend_id'\n",
    "for f_id in myFriendsIDs:\n",
    "    friend = api.get_user(f_id)\n",
    "    f_name = ''.join(s for s in friend.name if s in string.printable) # Cleanup non-printable chars\n",
    "    myFriendsNames[f_id] = f_name\n",
    "\n",
    "# Name of the latest added friend of Tobi\n",
    "myFriendsNames[myFriendsIDs[0]]"
   ]
  },
  {
   "source": [
    "After getting a list with the IDs from Tobis friends, we can go deeper! We crawl all friends of friends from Tobi and store them in a dict. This may take some time since Twitters API doesn't allow bigger API-calls like this, so there is at least one 15 min timeout after getting a RateLimitError. Take a cup of tea or do some sport while this executes."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get 2. Grade Follows (Friends)\n",
    "secondGradeFriends = {}\n",
    "print(f'Start crawling {len(myFriendsIDs)} Friends...')\n",
    "for f_id in myFriendsIDs:\n",
    "    try:\n",
    "        # Get friends of current friend (of Tobi) and stores them in secondGradeFriends dict\n",
    "        f_friends = api.friends_ids(f_id)\n",
    "        secondGradeFriends[f_id] = f_friends\n",
    "    except tweepy.RateLimitError:\n",
    "        # Prevents crashing if the 300 API-Call Limit from Twitters API caused an exeption\n",
    "        # Tries to continue in 15 Minutes again.\n",
    "        sleep_time = 15 * 60 + 1\n",
    "        now = datetime.datetime.now()\n",
    "        tend = now + datetime.timedelta(0, sleep_time)\n",
    "        print(f'Crawled already {len(secondGradeFriends)}!')\n",
    "        print(f'Current Time: {now.strftime(\"%H:%M:%S\")}')\n",
    "        print(f'Sleep for 15 Minutes (until {tend.strftime(\"%H:%M:%S\")}) to avoid RateLimitErrors. ')\n",
    "        time.sleep(sleep_time)\n",
    "        f_friends = api.friends_ids(f_id)\n",
    "        secondGradeFriends[f_id] = f_friends"
   ]
  },
  {
   "source": [
    "## Transforming data\n",
    "After waiting for twitter to hand over the data we are now able to create an edge list and a node list. The first approach is counting all overlapping friends of Tobis friends. E.g. Tobi has three friends: Ole, Christopher and Philipp. Ole and Christopher are both following (befriended with) Barack Obama and Elon Musk. So there would be an edge `Ole <--2--> Christopher`. Ole and Philipp are both following Michael Reeves, Alexandria Ocasio-Cortez and Greta Thunberg. This edge would be `Ole <--3--> Philipp`. All edges are after the counting saved to a file called `my-edgy-friends.edges` and all nodes are saved to `my-nody-friends.nodes`:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### String.join\n",
    "The String.join (e.g. ','.join) joins a list to a string seperated by the string. E.g. :\n",
    "```python\n",
    "mySeperatorString = '; '\n",
    "mySeperatorString.join(['Apple', 'Bee', 'Cat'])\n",
    "\n",
    "# or\n",
    "\n",
    "'; '.join(['Apple', 'Bee', 'Cat'])\n",
    "```\n",
    "Returns:\n",
    "```\n",
    "Apple; Bee; Cat\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reform data to fit into an edge list\n",
    "\n",
    "# Opens edge file with write access and write a head row\n",
    "f = open(\"my-edgy-friends.edges\", \"w\") # This time 'f' stands for 'file'\n",
    "f.write(','.join(['User ID', 'User ID', 'Number of overlapping friends (weight)']) + '\\n')\n",
    "\n",
    "# Array which stores already calculated combinations\n",
    "matched = []\n",
    "\n",
    "# I know, there is for sure a better way to do this, but its late and I want to go home. :)\n",
    "# Double iteration of myFriends, dont try to understand\n",
    "for f_id_i in myFriendsIDs:\n",
    "    f_friends_i = secondGradeFriends[f_id_i]\n",
    "\n",
    "    for f_id_j in myFriendsIDs:\n",
    "        # If i and j are same users or i and j in combination was already calculated continue with next one\n",
    "        if f_id_j == f_id_i or f'{f_id_j}-{f_id_i}' in matched:\n",
    "            continue\n",
    "\n",
    "        f_friends_j = secondGradeFriends[f_id_j]\n",
    "\n",
    "        # Number which counts the amount of overlapping friends\n",
    "        same_friends = 0\n",
    "\n",
    "        # Iterate through friends of i and j to count overlapping friends\n",
    "        for f_f_id_i in f_friends_i:\n",
    "            for f_f_id_j in f_friends_j:\n",
    "                if f_f_id_i == f_f_id_j:\n",
    "                    same_friends += 1\n",
    "        \n",
    "        # If overlapping friends exists write them to the edge list [id of i, id of h, number of overlapping friends]\n",
    "        if same_friends > 0:\n",
    "            f.write(','.join([str(myFriendsNames[f_id_i]), str(myFriendsNames[f_id_j]), str(same_friends)]) + '\\n')\n",
    "        \n",
    "        # Store the combination, so it doesn't calculated twice\n",
    "        matched.append(f'{f_id_i}-{f_id_j}')\n",
    "\n",
    "# Close file for os-security\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Names from friends\n",
    "f = open(\"my-nody-friends.nodes\", \"w\")\n",
    "f.write(','.join(['User ID', 'name']) + '\\n')\n",
    "for f_id in myFriendsIDs:\n",
    "    f.write(','.join([str(f_id), str(myFriendsNames[f_id])]) + '\\n')\n",
    "\n",
    "# Close file for os-security\n",
    "f.close()"
   ]
  },
  {
   "source": [
    "---\n",
    "## 2. Social Network Analysis\n",
    "This part can be executed without the 1. part!\n",
    "We are now able to read the data from the edge- and node files and put them in a dataframe:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # for analysing and changing data\n",
    "import networkx as nx # for creating edges and nodes\n",
    "import IPython # for showing html output in cells\n",
    "from pyecharts.charts import Graph # library for visualising Network\n",
    "from pyecharts import options as opts # further visualising options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Data into pandas DataFrame adjusting columns\n",
    "df = pd.read_csv(\"my-edgy-friends.edges\",names = [\"node1\", \"node2\", \"value\"] )\n",
    "df.drop(index=0, inplace=True) # Drop old header\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f'The Datframe has the dimension {df.shape[0]} rows and {df.shape[1]} columns'"
   ]
  },
  {
   "source": [
    "## Create the Network\n",
    "We can now extract these edges (rows) from our dataframe with the help of this function:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_edgelist(df: pd.DataFrame) -> [(str, str)]:\n",
    "    \"\"\"\n",
    "    takes:\n",
    "        a pandas datframe with target and source nodes in columsn\n",
    "    returns:\n",
    "        an edgelist in form of [(source, target)]\n",
    "\n",
    "    \"\"\"\n",
    "    # Iterating over the whole datframe and append the nodes to the edgelist\n",
    "    edgelist = []\n",
    "    for index, row in df.iterrows(): \n",
    "        edgelist.append([str(row.node1), str(row.node2)])\n",
    "    return edgelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgelist = create_edgelist(df)"
   ]
  },
  {
   "source": [
    "## Initiate the Network Graph\n",
    "The edges from the edgelist can now be added to a new nx.Graph object. The nx.Graph extracts all Nodes automaticly from the edges:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "G.add_edges_from(edgelist)\n",
    "G.nodes()"
   ]
  },
  {
   "source": [
    "With the help of nx it is now possible to calculate the `degree_centrality` for all nodes in our nx.Graph."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating degree_centrality which returns an object with the value for each node:\n",
    "# {'nodename'(str): degree_centrality_from_node(float), ...}\n",
    "centrality = nx.degree_centrality(G)\n",
    "# Making a Top 5 List regarding degree centrality\n",
    "centrality_top5 = sorted(centrality, key=centrality.get, reverse=True)[:5]\n",
    "centrality_top5"
   ]
  },
  {
   "source": [
    "## Visualize the Network Graph\n",
    "Now we can visualize the Network Graph. To do this we create two helper functions to configure appearence of the nodes and the edges."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_nodes_visualisation(G : nx.Graph)->[dict]:\n",
    "    \"\"\"\n",
    "    takes:\n",
    "        an networkx graph object\n",
    "    returns:\n",
    "        a list of dicts suitable for pyecharts network  visualisation nodes\n",
    "    \"\"\"\n",
    "    # Calculating the centrality degree of every node in the network and make a list out of the Top 5\n",
    "    centrality = nx.degree_centrality(G)\n",
    "    centrality_top5 = sorted(centrality, key=centrality.get, reverse=True)[:5]\n",
    "\n",
    "    # Create a new viz_nodes list from the Graph nodes with appearence information for each node\n",
    "    viz_nodes = []\n",
    "    for node in G.nodes():\n",
    "        # Fail-fast if the centrality of the node is to low. This minimizes the network which is very helpfull for large networks\n",
    "        if centrality[node] <= 0.001:\n",
    "            continue\n",
    "        \n",
    "        # Different appearence for the Top 5 nodes\n",
    "        if node in centrality_top5: #make another layout for top5 nodes\n",
    "            viz_nodes.append({\n",
    "                \"name\": node,\n",
    "                \"symbol\": \"triangle\",\n",
    "                \"symbolSize\": centrality[node]*10 ,\n",
    "                \"categorie\": \"top5\",\n",
    "                \"draggable\": True,\n",
    "                \"itemStyle\": {\"color\": \"#ff3f76\"},\n",
    "                \"label\": {\"show\": True},\n",
    "                \"value\": round(centrality[node],2)\n",
    "            })\n",
    "        else:\n",
    "            viz_nodes.append({\n",
    "                \"name\": node,\n",
    "                \"symbol\": \"rect\",\n",
    "                \"symbolSize\": centrality[node]*10,\n",
    "                \"categorie\": \"ordinary\",\n",
    "                \"draggable\": True,\n",
    "                \"itemStyle\": {\"color\": \"#789704\"},\n",
    "                \"label\": {\"show\": centrality[node] > 0.74}, # Show label only if centrality above 0.74\n",
    "                \"value\": round(centrality[node], 2)\n",
    "            })\n",
    "\n",
    "    return viz_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_edges_visualisation(G:nx.Graph)->[dict]:\n",
    "    \"\"\"\n",
    "    takes: \n",
    "        an networkx graph object\n",
    "    returns:\n",
    "        a list of dicts suitable for pyecharts network  visualisation edges\n",
    "    \"\"\"\n",
    "    # Create a new links list from the Graph nodes with appearence information for each link / edge\n",
    "    links = []\n",
    "    for index, row in df.iterrows():\n",
    "        links.append({\n",
    "            \"source\": str(row.node1),\n",
    "            \"target\": str(row.node2),\n",
    "            \"value\": int(row.value)\n",
    "        })\n",
    "    \n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = configure_nodes_visualisation(G)\n",
    "nodes[:2] # Example of the created list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = configure_edges_visualisation(G)\n",
    "links[:2] # Example of two edge dicts "
   ]
  },
  {
   "source": [
    "And with these configured links and nodes we can render a Graph Figure:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure object and add nodes and edges (called links) into the figure format layout, set labels for tooltip\n",
    "fig = Graph()\n",
    "fig.add(\n",
    "    \"Tobis Twitter Network\",\n",
    "    nodes,\n",
    "    links,\n",
    "    tooltip_opts = \"{a} <br> {b} : {c}\",\n",
    "    repulsion=0,\n",
    "    gravity= 1,\n",
    "    linestyle_opts = opts.LineStyleOpts(width=0.1, curve=0.1, color=\"source\")\n",
    ")\n",
    "fig.height = \"900px\"\n",
    "fig.width = \"900px\"\n",
    "fig.render_notebook()"
   ]
  },
  {
   "source": [
    "## Analysis\n",
    "But what can the Visualization tell us? Each link represents overlapping friends between two friends of Tobi. Therefore each link represents the homogeneity of the interests of those two friends. So, the centrallity of a node says how homogeneous the interests of the friend are compared to the rest of Tobis friends. In conclusion this Network shows us which accounts from Tobis Twitter friendslist have the most homogeneous interests compared to Tobi. So that can tell us which friend shares the relative most content which Tobi agrees on, but not which friend influences Tobi the most.\n",
    "\n",
    "---\n",
    "## 3. Alternative Approach\n",
    "To answer the question which friend of Tobi influences his Twitter Feed the most we must try another approach. This approach is to use a simple directed network. So here each edge represents a directed relationship between a 1st grade friend of Tobi and a 2nd grade friend of Tobi. E.g. Tobi has two friends (follows two accounts): Günni and Peter. Günni has two more friends, Sarah and Eli. Peter has also a friend: Erik. So there would be the edges: `Günni --> Sarah`, `Günni --> Eli` and `Peter --> Erik`. The edge list is saved to the file `my-edgy-friend-network.edges` but with just the account IDs since it would take to long to crawl Twitter to get the clear names of 20 000 people:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"my-edgy-friend-network.edges\", \"w\")\n",
    "f.write(','.join(['User ID', 'User ID']) + '\\n')\n",
    "\n",
    "for f_id in secondGradeFriends:\n",
    "    for f_f_id in secondGradeFriends[f_id]:\n",
    "        f.write(','.join([str(f_id), str(f_f_id)]) + '\\n')\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "source": [
    "_This can be executed with the saved edge-files from the repository without running the above code._\n",
    "\n",
    "Then we can import this data into a dataframe, create an edgelist, provide it to nx to create a nx.Graph obejct and calculate the centrality for each node:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_alt = pd.read_csv(\"my-edgy-friend-network.edges\", names = [\"node1\", \"node2\", \"value\"])\n",
    "df_alt.drop(index=0, inplace=True)\n",
    "df_alt = df_alt.fillna(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgelist_alt = create_edgelist(df_alt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_alt = nx.Graph()\n",
    "G_alt.add_edges_from(edgelist_alt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centrality_alt = nx.degree_centrality(G_alt)"
   ]
  },
  {
   "source": [
    "Now we can take the Top 20 Nodes and crawl Twitter for the clear names:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centrality_alt_top20_ids = sorted(centrality_alt, key=centrality_alt.get, reverse=True)[:20]\n",
    "top_20 = []\n",
    "for topID in centrality_alt_top20_ids:\n",
    "    friend = api.get_user(topID)\n",
    "    f_name = ''.join(s for s in friend.name if s in string.printable) # Cleanup non-printable chars\n",
    "    top_20.append(f_name)\n",
    "\n",
    "top_20"
   ]
  },
  {
   "source": [
    "So these are the 20 accounts which influence Tobis Twitter feed the most."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}